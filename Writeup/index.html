<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com?plugins=forms,typography"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            clifford: '#da373d',
          }
        }
      }
    }
  </script>
</head>
<body>

<div class="min-h-screen bg-gray-50 py-8 flex flex-col justify-center relative overflow-hidden lg:py-12">
<img src="https://play.tailwindcss.com/img/beams.jpg" alt="" class="fixed top-48 left-1/2 -translate-x-2/3 -translate-y-1/2 max-w-none" width="1308" />
<div class="absolute inset-0 bg-[url(https://play.tailwindcss.com/img/grid.svg)] bg-top [mask-image:linear-gradient(180deg,white,rgba(255,255,255,0))]"></div>
<div class="relative w-full px-6 py-12 bg-white shadow-xl shadow-slate-700/10 ring-1 ring-gray-900/5 md:max-w-3xl md:mx-auto lg:max-w-4xl lg:pt-16 lg:pb-28">
      
<article class="prose prose-slate mx-auto mt-8 lg:prose-lg">

<h1 class="text-indigo-800 underline decoration-sky-500">Robo Mapped</h1>
TODO. We will have a video here soon.

<h2 id="high-level-overview">High Level Overview</h1>
<p>Our goal with this project has been to create a remote control system to remotely explore
different areas of campus. This system mainly involves a tilt-based controller built using an Arduino
in addition to a wheeled robot, which can move forwards
or backwards or turn left or right based on the tilt of the controller. The robot can also
collect information about the world around it, including camera images, location data, and
rudimentary object detection for potential future autonomy. We have of course also included
an intuitive GUI web-client to display much of this data for the purposes of tracking the robot&#39;s
locomotive/kinematic history as well as its current state.</p>
<p>We envision that our system could be applied to explore dangerous or hard-to-reach areas, though in the
interests of completing a satisfactory integration for the class, we made choices which constrain the
areas where our robot is effective, such as using the Wifi-Scan Google API from lab to locate the robot.</p>
<h1 id="documentation">Documentation</h1>
<h2 id="components">Components</h2>
<ul>
<li>Robot (Driver/Sensor and Wifi-scanner/Locator)</li>
<li>Controller</li>
<li>Web Application</li>
</ul>
<h2 id="functionality">Functionality</h2>
<p>Functionality for our system is fairly intuitive and simple. Users press the start button (button 1) on the
controller
to begin moving the the robot. They then tilt forwards to move the robot forwards, backwards to move backwards, and to
either
side to move only one wheel (spinning the robot). "Forwards" for the robot always means with respect to its own
direction
and not that of the person controlling it (who may or may not be in another room, using the camera feed to guide
movement).
The user can disable movement by clicking button 1 again. They can open up our website to run the web client to see
the camera feed from the robot (at a rather slow framerate). On that same website, they can use the top menu bar to
select whether to switch to a historical map of the robot&#39;s trajectory or to save a set of times representing the start and end date/times of a path that is being recorded.</p>
<h3 id="start-robot-path">Start Robot Path</h3>
<p>We have an SQLite server on back-end that is updated on a consistent basis by the the robot. Every
couple milliseconds, the controller-robot system adds the latest kinematic data to the database, while
the every few seconds the robot performs a wifi scan to update the database with the latest
location data using a Google API.</p>
<h3 id="tilt-control-speeds-and-direction">Tilt control- speeds and direction</h3>
<p>We use a linearly interpolated and binned tilt-based controller. We began by modifying the ball simulation lab
code to not use a simulation, but instead to simply send its tilt information to another Arduino using a the ESP Now
module. We were able to use a simple Arduino ""script" sketch to display the unchanging MAC address of the
the robot&#39;s ESP32S2 Arduino, which enabled us to use the ESP Now to send tilt information every 80 ms. We chose this
number
because from experiment we found it to be responsive enough for a user to intuitively control the robot, and slow enough
to avoid clogging up bandwidth (which we also need for other features, like locating the robot).</p>
<h3 id="view-camera-feed-on-server">View Camera feed on Server</h3>
<p>We set up an Arducam on the robot to take small pictures of the space facing in front of they robot every couple seconds.
These are streamed up to the web server as base64 which then is stored in a camera database, also in SQLite.
Fortunately, our photos are small enough to fit.</p>
<h3 id="view-robot-paths-on-server-start-a-new-path-">View robot paths on server (start a new path)</h3>
<p>We have a database that stores the robot&#39;s locations as latitude and longitude plus an MIT building (or "off campus"), the former
being similar to the lab. We have a few endpoints for this information. One returns a table of the locations the robot
visited over time in sequence, just like in the lab. Another, returns a Google Maps map with waypoints where the robot
visited connected by polylines to provide a more intuitive and aesthetic interface. These are reachable from our
home
page, which is served by the 608 server. We also have a basic front end for starting and stopping (named) path recordings to save specific paths that may be of interest (for example pertaining to different users).</p>
<h3 id="auto-mode-object-detection-with-camera-feed">Auto Mode: object detection with camera feed</h3>
<p>The robot is able to detect whether objects are close to it, right in front. This data is meant mainly as an
exploration of what the robot will need in the future to be fully autonomous.</p>
<h2 id="system-diagram">System Diagram</h2>
<p><img src="sys1.png"></img></p>
<h2 id="system-layout">System Layout</h2>
<p>The physical system, barring the infrastructure for the server, is broken up into three pieces: the controller,
the robot&#39;s driver and sensor module, and the robot&#39;s wifi-scanning locator module. The robot&#39;s two modules are taped to
the
same chasis as if they were one. The robot has two modules because we
needed parallel Wifi-scanning due to it being slow and blocking ESP Now communication. We tried GPS and dual-core
parallelism but it was insufficient. More details on this will be available further below.</p>
<h3 id="the-robot-s-wifi-scanning-module">The robot&#39;s Wifi-Scanning Module</h3>
<p>The wifi scanning module is basically the same as that from the localization lab. We simply removed all other
unecessary
components. It has an Wifi-enabled ESP32 connected to power and ground contacting our API endpoints on a consistent
basis.</p>
<h3 id="the-robot-s-driver-module">The robot&#39;s Driver Module</h3>
<p>The robot&#39;s driver module is altogether different from anything in any lab. It is connected to a pair of motors which
are
connected to wheels, all on a chasis which also holds two battery packs: one for the the motors and one for the
Arduino. We have a motor driver to control the motors and an Arducam to take photos.</p>
<h3 id="controller">Controller</h3>
<p>The controller is similar to the ball-sim lab but with all unecessary components removed. It has a couple buttons
(though it only needs one) to enable or disable remote control. On top of this it has an IMU and an ESP32 with
ESP Now enabled.</p>
<h2 id="hardware-layout">Hardware Layout</h2>
<h3 id="the-robot-esp-now-reciever-">The Robot (ESP Now Reciever)</h3>
<p><img src="recv.png"></img></p>
<h3 id="controller">Controller</h3>
<p><img src="send.png"></img></p>
<h2 id="parts-list">Parts List</h2>
<ul>
<li>Hobbyist Motors (from TA)</li>
<li>Hobbyist Wheels (from TA)</li>
<li>Hobbyist Chasis (from TA)</li>
<li>AA Battery Pack (from TA)</li>
<li>4x AA 1.5V Batteries</li>
<li>Dual Core ESP32S2 Arduino (3x)</li>
<li>Camera (ArduCam)</li>
<li>Sonar (HCSR04, used for aesthetics)</li>
<li>5v Battery Packs (2x)</li>
<li>L298 Motor Driver</li>
<li>IMU (MPU 6050)</li>
<li>USB and jumper cables</li>
<li>Button (From Lab)</li>
</ul>
<h2 id="total-parts-price">Total Parts Price</h2>
<p>Around $63.31 excluding hardware from class and the motors, wheels, chasis,
and AA battery pack, which our TA, Amadou, was generously able to lend us
for the duration of this project.</p>
<h2 id="other-components">Other Components</h2>
<ul>
<li>Server (software)</li>
<li>ESP Now (built-in to the ESP32S2 Arduino)</li>
<li>Dual Core Chip (built-in to the ESP32S2 Arduino)</li>
</ul>
<h2 id="design-challenges-and-decisions">Design Challenges and Decisions</h2>
<p>We faced various challenges throughout the design and implementation process.
Many of them we were forced to simply work around. They mainly included, but
were not limited those below.</p>
<h3 id="implementation-challenges">Implementation Challenges</h3>
<p><strong>GPS not working.</strong> We had troubles setting up our GPS. We followed the guide posted on the 6.08 class site, but
its
pins were outdated. After discussing with a TA, we were able to find the right pins, but even then it would not
display
anything. We tried all permutations of wiring for our GPS module, but were unable to fetch location information even
with it flashing as if it were functional. For that reason we abandoned GPS in favor of wifi-scanning.</p>

<p><strong>Wifi-scanning and requests latency; dual-core parallelism bugs with the Wifi Module.</strong> Wifi-scanning from the lab
was
too slow for our needs, since it blocked the robot from acting on controller requests to change speed. For this reason
we
first tried using the dual-core functionality of the ESP32S2 Arduino, but it introduced parallelism bugs
sporadically
and unpredictably when the Wifi module did a scan to update its information regarding nearby access points (to query
Google's API with). This forced us to simply use a third Arduino for our location data collection.</p>

<p><strong>Serving a react-based web client on the server.</strong> Our goal was to create a reactive
web-client for our application so that we could enable real-time data and control of the robot. 
However, we had to create a client-server system that would be compatible with the 6.08 server, which
has certain limitations in which content type it can serve. We initially tried to use an HTML file and inject 
Javascript with our main app React code, but we realized that we had to provide access to other Javascript files 
to support React functionality such as state management and styling. Our next attempt involved using "bundle" commands
in order to bundle all our React files into one file that could be server on the server. This did not work either, so 
we decided to look into features that React provides for this purpose. In doing that, we learned that React "build"
packages the app in a single file and provides a PUBLIC_URL to access it at. With this, we were able to access
the public build from the 6.08 server, and serve our entire app, including different routes, files, and packages. </p>

<p><strong>Storage space for camera data both on on-board buffers and in the server.</strong> Most image sizes were too big,
especially in base64, to store on the Arduino and send to the server in one request. We also worried at first that these images
would not fit into our database. Fortunately, this was not the case and we were able to do both of these things by
picking the smallest possible image size, 160x120.</p>

<p><strong>Lack of power for a sensor-laden robot.</strong> We were unable to do an end to end test until week 3 of the project
because
the robot had too many sensors to power them all in addition to the motors, even when given 6V from the battery pack
through
the motor driver. Luckily, our order of 5V USB power supplies arrived in time for us to power the Arduino
independently
with a power supply and thereby make our robot autonomous (disconnecting it from the computer) and able to power its
sensors.</p>

<p><strong>Arducam debugging.</strong>
We first tested the camera with the example script. Then we base64 encoded the image and sent a post request to the server. 
There were quite a few issues we faced including running out of buffer size so we tried PSram but our Arduino didn’t have the Wrover ESP32 chip so we ended up 
reducing the resolution to 160x120 which is sufficient for future object detection. 
Then on the server side, we created a new database that stores 1 entry of the timestamp and base 64 encoding of the image. 
We also decoded the base 64 encoding and saved the image as a jpg which initially would get overwritten to avoid database space constraints, 
but eventually we were able to save multiple images.
</p>

<p><strong>JPEG HTML embedding.</strong> We encountered challenges in serving images on our front-end from the database, which 
were even harder to tackle because of the variety of software and hardware components involved in this feature.
We initially confirmed that the Arduino camera was properly taking images by decoding the base64 image strings
on our local computers and successfully viewing the images. However, our HTML server would attempt to decode
the same image strings and would not render a valid image. We debugged this by using our browser console and 
adapting the image data source component to include or exclude different sections, such as content-type specifications 
and other attributes. We started to clip certain parts of the base64 data string itself, since we noticed it had 
extraneous characters at the beginning or end. We eventually found that our hardware code was storing image data strings 
with padding characters at the end, separated by a "=" symbol from the rest of the valid base64 characters. For some 
reason, our local file server was able to parse these anyways, but our HTML server would not render the image with this 
suffix. Finally, we implemented parsing code to make sure the front-end server is receiving valid <code>base64</code> image 
source strings from the database.</p>

<p><strong>Low Camera Framerate.</strong> Because it took the Arduino a lot of work to take the pictures, encode them in base64,
send
them to the server, await an OK as the server input them into the database, and so forth, the frame rate for our
camera
is very low. It is enough for real-time control in static settings, but not viable for dynamic settings (i.e. with
many
moving objects). The TAs told us that this was as good as we were gonna get from the Arducam with the ESP32, but we
imagine that with better hardware and some performance optimization (and streaming) it would be possible to have a
more
real-time feed.</p>

<p><strong>Object detection model and API use.</strong>

We went about a few methods for object detection. One was creating our own model, which worked, but since we couldn’t download tensorflow on the 
server we restarted and instead used a google vision API from the server. We went through a number of authentication issues with private keys etc. but ultimately were 
able to compute the object closest to the camera and its coordinates and save this object in the database. 
It is also sent to the TFT of the sender so the sender can now decide whether they want to look at the camera or just 
their TFT screen when controlling the robot. As a simple add-on this would enable us to stop the car if a person is detected in sight (to create an “auto-mode”). </p>

<p><strong>Speed data collection.</strong>
Since to get real time data, we ran the speed collection loop in parallel (on one of the two cores) to all server requests, we ran into some parallelism issues that caused the Arduino to 
intermittently crash for unknown reasons. We had to temporarily disable wifi scanning on the ESP, until we moved wifi scanning only to the robot.
</p>

<p><strong>Path data sending.</strong>
The challenges for this aspect included working with large amounts of data to understand the format it was coming in, such as through well placed console.log messages or using API tools to 
check the output of API endpoints. It also included trying out different intervals of data retrieval that would be reasonable yet not slow down the UI significantly. 
</p>

<h3 id="design-decisions">Design Decisions</h3>

<p><strong>Map front-end path rendering.</strong>
 We used google map's API to show a "polyline" in between each pair of adjacent points. We also refactored the code in order to properly refresh and load the coordinates. 
 Before, the webapp was pushing all new points into an array constantly, which made the webapp very slow. Now, we have a separate async function that gets the data from the API, 
 reformats it into the right format (array of coordinates rather than dictionary of array of lat, array of lng, etc.) and sends the full updated data to the server. 
 This also allows us to only send the most recent set of coordinates into the UI component.
</p>

<p><strong>Robot movement control.</strong> After some exploration and risk assessment, we decided to proceed with tilt control rather than gesture control since gesture control 
    brings about challenges such as reliable detection of relative movement and individual, independent motions. Tilt control would be 
more feasible and reasonable with the scope of our project, and had proven to be robust and reliable with our other class projects. </p>

<p><strong>Location detection.</strong>
Our implementation challenges are what led us to choose wifi-scanning over GPS as our localization technique. We
understand that requiring Wifi is potentially not ideal for the sort of hypothetical scenarios in which a robot like this would be useful, but
understand that to create our MVP it was necessary to use the tools at our disposal.
We decided that instead of using the GPS module, we were going to run 2 ESPs in parallel on the robot to load balance the work between,
one one hand, locomotion and sensors, and on the other, wifi scanning and certain HTTP/HTTPS requests (the latter of which are very slow, and blocking).
We'd also tried to use the dual core for this purpose initially, but it led to parallelism bugs likely due to shared resources (we got Guru meditation
errors sporadically for illegal writes to, supposedly, shared memory). Using 2 ESPs solved our problems, so we settled with that strategy.
</p>

<p><strong>Path photo history.</strong>
In order to correlate images with locations along a path, we designed an API endpoint that returns an array of images from the database and also the information about where 
and when the image was taken. It does this by querying the location database with a certain time range based on the image datetime for each image. The main challenge 
was developing an efficient design for this and deciding to minimize the location data we query for on each database query by looping through just the images we need. 
</p>

<p><strong>UI design decisions.</strong>
We also developed a front-end function that returns multiple images from an API endpoint, and further learned how to enable seamless array data transfer from the server.py file to a 
front-end JavaScript file with JSON parsing, allowing it to render continuously and with negligible delay.
</p>

<p><strong>Showing path history.</strong>
We store a path using the start and end timestamps for that path, and had to decide how to display this data. We created a simple frontend in react that is a static table which you 
interact with using url parameters, for flexibility and easy access. We decided to limit the table to 20 entries for readability, and it only shows times after the start time and before the end time.
</p>

<p><strong>Binned speeds.</strong>
Inspired by the napster lab, we binned the speeds of the robot. We added minimum tilts in each axis necessary to enable movement and then broke up the speeds into quartiles. This would allow our robot to move at different speeds. 
We used a linear interpolation/mapping from the space of tilts to the space of speeds (with clipping for a max tilt of around 1G at 180 degrees and min tilt of that but negative).
In the process of doing this, to get real time data, we also ran the loop in parallel (on one of the two cores) to all server requests because otherwise they would block and ruin the user experience. 
We were able to combine the binned-speed controller plus the robot by using the battery pack for the controller. 
</p>

<h1 id="detailed-code-layout">Detailed Code Layout</h1>
<h2 id="server">Server</h2>
<p>Our server code is seperated into two modules, called <code>Crud</code> and <code>Webpage</code>, which respectively handle API endpoints
to read/write from/to the databases/state and a simple webpage server. We have a request handler which based on the URL
arguments passed is able to route the request to either Crud or the Webpage&#39;s handlers.</p>
<p>The Crud endpoints modify three databases: one for camera data, one for location data, and one for kinematic data.
We have split it up this way because the modules handling the sending of these different types of data have different
frequencies at which they send their data. The Crud object implements entirely static methods which are typically
wrapped in a python SQLite connection and cursor decorator to modify the correct database. Using decorators like this
along with the static objects was able to make our code a lot cleaner and easier to work with. Below is an example Code
snippets to give you a sense of what modifiying our databases looks like.</p>
<pre><code class="lang-~~~"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Crud</span><span class="hljs-params">(object)</span>:</span>
    LOC_FILE = <span class="hljs-string">"/var/jail/home/team10/loc.db"</span> 
    ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">withConnLocCursor</span><span class="hljs-params">(func: Callable[[sqlite3.Cursor, sqlite3.Connection, Any], str])</span> -&gt; str:</span>
        <span class="hljs-string">""" Wrap your functions in this when you want them to have access to the database"""</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">wrapper</span><span class="hljs-params">(*args, **kwargs)</span>:</span>
            conn = sqlite3.connect(Crud.LOC_FILE)
            c = conn.cursor()
            result = func(c, conn, *args, **kwargs)
            conn.commit()
            conn.close()
            <span class="hljs-keyword">return</span> result
        <span class="hljs-keyword">return</span> wrapper
    ...

<span class="hljs-meta">    @withConnLocCursor </span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">handle_whereami</span><span class="hljs-params">(c: sqlite3.Cursor, conn: sqlite3.Connection, request: Any)</span> -&gt; str:</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-string">"x"</span> <span class="hljs-keyword">in</span> request[<span class="hljs-string">"values"</span>] <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-string">"y"</span> <span class="hljs-keyword">in</span> request[<span class="hljs-string">"values"</span>]:
            <span class="hljs-keyword">return</span> <span class="hljs-string">"Error: please provide x and y"</span>
        x_str: str = request[<span class="hljs-string">"values"</span>][<span class="hljs-string">"x"</span>]
        y_str: str = request[<span class="hljs-string">"values"</span>][<span class="hljs-string">"y"</span>]

        <span class="hljs-keyword">try</span>:
            x: float = float(x_str)
            y: float = float(y_str)
            loc: str = GeoFencer.get_area((x, y))
            now = datetime.now()
            <span class="hljs-comment"># post to database </span>
            c.execute(<span class="hljs-string">"""CREATE TABLE IF NOT EXISTS loc_data (time_ timestamp, x_x real, y_y real, build text);"""</span>)
            c.execute(<span class="hljs-string">'''INSERT into loc_data VALUES (?,?,?,?);'''</span>, (now, x,y,loc))
            <span class="hljs-keyword">return</span> loc 
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            <span class="hljs-keyword">return</span> f<span class="hljs-string">"Error: please provide x and y as floats, had error: {e}"</span>
    ...
</code></pre>
<p>The following endpoints are available for Crud:</p>
<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> Endpoint and Type</th>
            <th style="text-align:left"> URL Query String </th>
            <th style="text-align:left"> Description </th>
        </tr>
        <tr>
            <th style="text-align:left"> Kinematic and Location API GET </th>
            <th style="text-align:left"> Any Value other than &quot;camera,&quot; &quot;monalisa,&quot; &quot;whereami,&quot; or &quot;Wherehaveibeen&quot;
            </th>
            <th style="text-align:left"> Get a list of historial location and kinematic data of the robot. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Kinematic and Location API POST </th>
            <th style="text-align:left"> None </th>
            <th style="text-align:left"> Post a new entry of location and kinematic data. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Whereami (GET) </th>
            <th style="text-align:left"> ?whereami=1 </th>
            <th style="text-align:left"> Find out where a latitude and longitude are at MIT. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Wherehaveibeen (GET) </th>
            <th style="text-align:left"> ?wherehaveibeen=1 </th>
            <th style="text-align:left"> Get a static HTML table of where the robot has been.</th>
        </tr>
        <tr>
            <th style="text-align:left"> Camera POST </th>
            <th style="text-align:left"> ?camera=1 </th>
            <th style="text-align:left"> Post a picture to the camera database. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Camera GET </th>
            <th style="text-align:left"> ?camera=1 </th>
            <th style="text-align:left"> Get an HTML page with a picture from the latest camera feed. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Camera All GET </th>
            <th style="text-align:left"> ?allcamera=1 </th>
            <th style="text-align:left"> Get an serialized sequence of images in base64. </th>
        </tr>
    </tbody>
</table>

<p>The webpage only has one single meaningful endpoint to return the string representing the <code>index.html</code> of our
webpage. We also have a dummy &quot;mona lisa&quot; endpoint accessible with <code>?monalisa=1</code> as a sample of HTML base64 embedding of
images. Note that we had to put the wherehaveibeen endpoint in Crud and not Webpage because it requires reading from the
database directly. We implemented it as a low-level, simple MVP for localization early on, before we had map
waypoints.</p>
<p>Lastly, we have a GeoFencer object which encapsulates the functionality from the GeoFencing lab and allows us to
locate the robot on MIT.</p>
<h2 id="database-layout">Database Layout</h2>
<h3 id="location-database-with-example">Location Database with Example</h3>
<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> &quot;time_&quot; Time (timestamp)</th>
            <th style="text-align:left"> &quot;x_x&quot; Latitude (real)</th>
            <th style="text-align:left"> &quot;y_y&quot; Longitude (real)</th>
            <th style="text-align:left"> &quot;build&quot; Building (text)</th>
        </tr>
        <tr>
            <th style="text-align:left"> 2022-05-08 08:15:27.243860 </th>
            <th style="text-align:left"> 42.360226</th>
            <th style="text-align:left"> -71.094139</th>
            <th style="text-align:left"> Student Center</th>
        </tr>
    </tbody>
</table>

<h3 id="camera-database-with-example">Camera Database with Example</h3>
<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> &quot;time_&quot; Time (timestamp)</th>
            <th style="text-align:left"> &quot;image&quot; Image Base64 (text)</th>
            <th style="text-align:left"> &quot;response&quot; Object Detection Class (text)</th>
        </tr>
        <tr>
            <th style="text-align:left"> 2022-05-08 08:15:27.243860</th>
            <th style="text-align:left"> /9j/... </th>
            <th style="text-align:left"> person </th>
        </tr>
    </tbody>
</table>

<h3 id="kinematics-database-with-example">Kinematics Database with Example</h3>
<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> &quot;time_&quot; Time (timestamp)</th>
            <th style="text-align:left"> &quot;a_x&quot; Tilt x (real)</th>
            <th style="text-align:left"> &quot;a_y&quot; Tilt y (real)</th>
            <th style="text-align:left"> &quot;v_x&quot; Deprecated (real)</th>
            <th style="text-align:left"> &quot;v_y&quot; Deprecated (real)</th>
            <th style="text-align:left"> &quot;speed&quot; Speed (real)</th>
            <th style="text-align:left"> &quot;direction&quot; Direction (real)</th>
        </tr>
        <tr>
            <th style="text-align:left"> 2022-05-08 08:15:27.243860</th>
            <th style="text-align:left"> 0.534 </th>
            <th style="text-align:left"> -0.02 </th>
            <th style="text-align:left"> 0 </th>
            <th style="text-align:left"> 0 </th>
            <th style="text-align:left"> 75</th>
            <th style="text-align:left"> &quot;1 </th>
        </tr>
    </tbody>
</table>

<h2 id="web-client">Web Client</h2>
<h3 id="react">React</h3>
<p>We were able to use ReactJS with Typescript to create an aesthetic and reactive experience for the users of our webpage.
Our main components are a map, a plotter of tilt and kinematic data, and a camera handler that shows a historical,
real-time view of the camera feed from the robot. The map communicates with a Google API for maps to enable us to place
pins and create paths, which helps visualize what the robot does.</p>
<p>The other major advantage of React is that enables us to do single-page routing. This is ideal for us because deployment
on the python server (which is not meant to serve multipage applications) was non-trivial, and thus having a stateful
single-page application handles these engineering challenges on the client side more easily.</p>
<h3 id="layout-handling">Layout Handling</h3>
<p>We used Tailwind CSS to generate utility classes which handle all of the styling on the webpages. Tailwind CSS is unique
in that it helped with tricky layout handling for the canvas, especially the Google API map.</p>
<h3 id="deployment">Deployment</h3>
<p>The main insight for deploying reactive applications using a build tool like webpack from the 6.08 server is that you
need to staticaly bundle everything somewhere where it can be reached, an then you need to change the paths in your
javascript file to include the full path on the 6.08 server. We were able to do this by changing environment variables
in a secrets file during build time. Changing the secrets file on the server itself is also important because if students are using an API key like we were for
the Google Map, then it is important that they avoid writing it in their javascript file due to security concerns. This
is not obvious to someone new to web development, but it is important to understand.</p>
<h3 id="challenges">Challenges</h3>
<p>We wish we&#39;d had Git or in general version control already built-in to the server so that we could collaborate more
easily there. This is meaningful because we needed to test both our front-end and back-end in real time to debug and we
ran into issues due to people pushing things to production randomly for the purposes of testing.</p>
<p>Another major challenge was synchronizing the location stream and the images from the database without taking too long
and blocking on the client. We were able to surmount this by lowering the frequency of updates on the client, which
fortunately was not perceptibly slower (due to the camera being, in itself, slow).</p>
<p>Lastly, we had to work around bugs involving react&#39;s native BrowserRouter, because we couldn&#39;t use paths ont the client.
Instead, we used a state machine (similar to those we&#39;ve seen throughout the class) to select which page to show. Each
page has a state (roughly) and each state transitions to each other using a button in the header of our web client. We
conditionally render based on the state.</p>
<h2 id="esp32-controller-sender-">ESP32 Controller (Sender)</h2>
<b>Sender.ino: /arduino/sender/sender.ino</b>
<p>
he ESP32S2 controller is fortunately one of the simpler Arduino implementations in our project. Initially it was
doing
all the localization work because our Robot did not have enough power to use Wifi, but once we were able to shift
that
to be on-board in the robot, we were able to just handle ESP Now messaging. Every 80ms, our code simply gets the tilt
(corresponding to speed), processes it, then sends it to the robot (with some additional data like direction, angle, and
so
forth, though we only support the four main directions, forward, backwards, left and right, denoted by UP = 1, DOWN
= 2,
LEFT = 3, RIGHT = 4, for the purpose of keeping a de-noised experience; we also have NONE signifying not moving).</p>
<p>The <code>OnDataSent</code> function is a callback which ESP Now calls when it successfully sends a message. It is used to
inform
the user through the Serial console (if present) that data was successfully sent (which is important for debugging),
but
is otherwise not that important. The functions <code>get_direction</code>, <code>get_speed</code>, and <code>get_angle</code> turn tilt data into a
direction and a speed for that direction. As previously mentioned there are four directions for the four rays coming
out
of the origin of the two axes on the 2D plane corresponding to the birds-eye view of the ground. We also have a
functionality to print these kinematics data to the TFT so the user can know how they are conrolling the robot (for
example if it&#39;s far away).</p>
<p>We bin our speeds into 4 bins based on a linear mapping from the range of tilts (between 0 and 1G) and the range of
speeds (between 0 and 100 in a unit pre-determined by our motor driver). This is similar to the work done in the
Napster
lab for the notes, except the function is in fact purely linear.</p>
<h2 id="esp32-robot-driver-and-sensor-">ESP32 Robot (Driver and Sensor)</h2>
<b>Receiver_w_cam.ino</b>
<p>This file includes both camera detection and posting as well as receiver code for esp now communication and converting speed and direction to robot instructions with move_car. For camera detection we set resolution to 160x120 to not overflow space and base64 encoded each frame before posting to the camera database. Additional parsing was done on the server. 
The file also included esp now code through OnDataRecv to receive the x,y tilt info and associated stats present in the info variables for direction and speed (which are needed later for moveCar.)    </p>
<p>Support functions: <b>/arduino/receiver_w_cam/camera_support.h</b>. These include primarily base64 encoder functions. </p>
<h2 id="dual-core-tinyml">Bonus: TinyML</h2>
<p>We explored a few different methods of object detection since we wanted to first create something that was more customizable rather than using an API so we tried tinyML. TinyML uses the "Eloquent TinyML" library on the Arduino to do basic detection of objects and other such machine learning tasks. We used their implementation to create our working object detection. However, we ended up shifting to an API because it was too hardware intensive to run on the edge.</p>
<h2 id="esp32-robot-wifi-scanner-locator-">ESP32 Robot (Wifi-Scanner/Locator)</h2>
<p>The Wifi-scanner code is similar to the lab. It simply runs on a loop to scan wifi networks, build the query JSON,
and
then send it to the Google API at <code>https://www.googleapis.com/geolocation/v1/geolocate</code> with the key provided to us
in
one of the labs. Once it is done with this, it sends a request to our backend to update our locations database.</p>
<h1 id="milestone-videos">Milestone Videos</h1>
<h2 id="week-1">Week 1</h2>
<h3 id="esp-now">ESP Now</h3>
<p><a class="underline decoration-indigo-400" href="https://youtube.com/shorts/_qTj1VqcwLc?feature=share">Short is available here.</a>
For some reason Youtube kept insisting that we make this a short.</p>
<h3 id="database">Database</h3>
<p><a class="underline decoration-indigo-400" href="https://youtube.com/shorts/Jvd1F6fFxmg?feature=share">Short is available here.</a>
For some reason Youtube kept insisting that we make this a short.</p>
<h3 id="-development-website">(Development) Website</h3>
<p><a class="underline decoration-indigo-400" href="https://youtube.com/shorts/j2ca18ya4Ws?feature=share">Short is available here.</a>
For some reason Youtube kept insisting that we make this a short.</p>
<h3 id="typescript-and-better-plotting">Typescript and Better Plotting</h3>
<!-- who the heck uses loom though -->
<div style="position: relative; padding-bottom: 64.5933014354067%; height: 0;"><iframe class="w-full aspect-video" 
        src="https://www.loom.com/embed/9cb2908149bb456d852604ae41a1b3b7" frameborder="0" webkitallowfullscreen
        mozallowfullscreen allowfullscreen
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>

<h2 id="week-2">Week 2</h2>
<h3 id="-production-website">(Production) Website</h3>
<iframe class="w-full aspect-video" width="560" height="315" src="https://www.youtube.com/embed/rjyxQVHdtcE" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

<h3 id="locating-app-in-mit-using-wifi-and-google-api">Locating App in MIT using Wifi and Google API</h3>
<iframe class="w-full aspect-video" width="560" height="315" src="https://www.youtube.com/embed/FPgzCNh0eX4" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

<h3 id="path-on-webserver">Path on Webserver</h3>
<iframe class="w-full aspect-video" width="560" height="315" src="https://www.youtube.com/embed/RTUxWQrvx-w" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

<h3 id="moving-robot-s-wheels-non-autonomous-">Moving Robot&#39;s Wheels (Non-Autonomous)</h3>
<p>This got like 400 views on Youtube.</p>
<iframe class="w-full aspect-video" 
    src="https://player.vimeo.com/video/707954511?h=92e5081bfd&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
    frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen
    title="wheels1.MOV"></iframe>

<h2 id="week-3">Week 3</h2>
<h3 id="binned-speeds">Binned Speeds</h3>
<iframe class="w-full aspect-video"  width="560" height="315" src="https://www.youtube.com/embed/lkiynghh2tU" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

<h3 id="moving-the-robot-autonomous-">Moving The robot (Autonomous)</h3>
<iframe class="w-full aspect-video" 
    src="https://player.vimeo.com/video/707954440?h=c64497cc93&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
    frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen
    title="integrated.MOV"></iframe>

<h3 id="react-router-and-porting-to-react">React Router and Porting to React</h3>
<iframe class="w-full aspect-video"  src="https://drive.google.com/file/d/19HDeVBCRObiUB72gXjIIG6Xk1g50N2y0/preview" width="640" height="480"
    allow="autoplay"></iframe>

<h3 id="camera-feed-on-website">Camera Feed On Website</h3>
<div style="position: relative; padding-bottom: 62.5%; height: 0;">
    <iframe class="w-full aspect-video" 
        src="https://www.loom.com/embed/e42725fd7e3c4c4d9e5d2d0830054df8" frameborder="0" webkitallowfullscreen
        mozallowfullscreen allowfullscreen
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>

<iframe class="w-full aspect-video"  src="https://drive.google.com/file/d/1iCJSLg03OSt5NRfpT1u9LspFbheg6mrJ/preview" width="640" height="480"
    allow="autoplay"></iframe>

<h2 id="week-4">Week 4</h2>
<h3 id="robot-localization-from-robot-using-gps-or-alternative-method">Robot Localization (from Robot) using GPS or Alternative Method</h3>
<p>You can find our short <a class="underline decoration-indigo-400" href="https://www.youtube.com/shorts/DohrP9NixmE">here</a>.</p>
<h3 id="dynamic-camera-feed">Dynamic Camera Feed</h3>
<p>We have a dynamic camera feed available in the website we made. It is created by doing a, metaphorically speaking, inner
join between the camera and location data.</p>
<div style="position: relative; padding-bottom: 62.5%; height: 0;">
    <iframe class="w-full aspect-video" 
        src="https://www.loom.com/embed/352e227e9c99414fab7271bbdc71d600" frameborder="0" webkitallowfullscreen
        mozallowfullscreen allowfullscreen
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>

<p>We also have the integrated behavior here:</p>
<iframe class="w-full aspect-video" src="https://drive.google.com/file/d/1swHPeBE5sORZbmmahVKGt5c6eY5GmhDY/preview" width="640" height="480"
    allow="autoplay"></iframe>

<h3 id="paths-travelled-on-database">Paths Travelled on Database</h3>
<iframe class="w-full aspect-video"  width="560" height="315" src="https://www.youtube.com/embed/VZjazDh3RV4" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

<h3 id="object-detection-and-display">Object Detection and Display</h3>
<iframe class="w-full aspect-video"  src="https://drive.google.com/file/d/1RZgU-t1SUoRDS1I6tiApG_-eogOuQYkN/preview" width="640" height="480"
    allow="autoplay"></iframe>

<h2 id="end-to-end-integration-using-the-robot">End to End Integration: Using The Robot</h2>
<p>TODO. This will come soon.</p>
<h1 id="historical-notes">Historical Notes</h1>
<p>We initially thought of making a mostly web-based project involving a swiping mechanic to catch turtles on a web
client
by moving the arduino (sort of like nintendo kinect). The design of that idea is fleshed out in the project proposal
we
submitted 4 weeks ago. However, Joe and the TAs dissuaded us from pursuing such an idea because doing localization
of
the Arduino in 3D space using an IMU is an unsolved problem and beyond our scope. This is why we opted to instead do
tilt control for our robot. Few of us have Robotics experience and we thought this would be an exciting and fruitful way to learn about it. Fortunately, despite many challenges, we have found it to be so.</p>
<h1 id="team-members">Team Members</h1>
<ul>
<li>Natasha Maniar</li>
<li>Adriano Hernandez</li>
<li>Sualeh Asif</li>
<li>Daniela Velez</li>
</ul>

</article>

</div>
</div>
</body>
</html>