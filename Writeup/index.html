<!-- <head>
    <style>
        table {
            font-family: arial, sans-serif;
            border-collapse: collapse;
            width: 100%;
        }

        td,
        th {
            border: 1px solid #dddddd;
            text-align: left;
            padding: 8px;
        }

        h2 {
            font-family: arial, sans-serif;
        }

        body {
            padding: 40px;
        }

        tr:nth-child(even) {
            background-color: #dddddd;
        }
    </style>
</head> -->

<div style="font-size: 30; font-weight: bold; text-align: center;">Robo Mapped</div>
TODO. We will have a video here soon.

# High Level Overview
Our goal with this project has been to create a remote control system to remotely explore
different areas of campus. This system mainly involves a tilt-based controller built using an Arduino
in addition to a wheeled robot, which moves can move forwards
or backwards or turn left or right based on the tilt of the controller. The robot can also
collect information about the world around it, including camera images, location data, and
rudimentary object detection for potential future autonomy. We have of course also included
an intuitive GUI web-client to display much of this data for the purposes of tracking the robot's
locomotive history as well as its current state.

We envision that our system could be applied to explore dangerous or hard-to-reach areas, though in the
interests of completing a satisfactory integration for the class, we made choices which constrain the
areas where our robot is effective, such as using the Wifi-Scan Google API from lab to locate the robot.

# Documentation

## Components
- Robot (Driver/Sensor and Wifi-scanner/Locator)
- Controller
- Web Application

## Functionality
Functionality for our system is fairly intuitive and simple. Users press the start button (button 1) on the
controller
to begin moving the the robot. They then tilt forwards to move the robot forwards, backwards to move backwards, and to
either
side to move only one wheel (spinning the robot). ``Forwards'' for the robot always means with respect to its own
direction
and not that of the person controlling it (who may or may not be in another room, using the camera feed to guide
movement).
The user can disable movement by clicking button 1 again. They can open up our website to run the web client to see
the camera feed from the robot (at a rather slow framerate). On that same website, they can use the top menu bar to
select whether to switch to a historical map of the robot's trajectory.

### Start Robot Path
We have an SQLite server on back-end that is updated on a consistent basis by the the robot. Every
couple milliseconds controller-robot system adds the latest kinematic data to the database, while
the every few seconds the robot performs a wifi scan to update the database with the latest
location data using a Google API.

### Tilt control- speeds and direction
We use a linearly interpolated and binned tilt-based controller. We began by modifying the ball simulation lab
code to not use a simulation, but instead simply send its tilt information to another Arduino using a the ESP Now
module. We were able to use a simple Arduino ``script'' sketch to display the unchanging MAC address of the
the robot's ESP32S2 Arduino, which enabled us to use the ESP Now to send tilt information every 80 ms. We chose this
number
because from experiment we found it to be responsive enough for a user to intuitively control the robot, and slow enough
to avoid clogging up bandwidth (which we also need for other features, like locating the robot).

### View Camera feed on Server
We set up an Arducam on the robot to take small pictures of the space facing in front of they robot eery couple seconds.
These are streamed up to the web server as base64 which then is stored in a camera database, also in SQLite.
Fortunately, our photos are small enough to fit.

### View robot paths on server (start a new path)
We have a database that stores the robot's locations on the MIT campus, as well as in latitude and longitude, the former
being similar to the lab. We have a few endpoints for this information. One returns a table of the locations the robot
visited over time in sequence, just like in the lab. Another, returns a Google Maps map with waypoints where the robot
visited connected by polylines to provide a more intuitive and aesthetic interface. These are reachable from our
home
page, which is served by the 608 server.

### Auto Mode: object detection with camera feed
The robot is able to detect whether objects are close to it, right in front. This data is meant mainly as an
exploration of what the robot will need in the future to be fully autonomous.

## System Diagram
<img src="sys1.png"></img>

## System Layout
The physical system, barring the infrastructure for the server, is broken up into three pieces: the controller,
the robot's driver and sensor module, and the robot's wifi-scanning locator module. The robot's two modules are taped to
the
same chasis as if they were one. The robot has two modules because we
needed parallel Wifi-scanning due to it being slow and blocking ESP Now communication. We tried GPS and dual-core
parallelism but it was insufficient. More details on this will be available further below.

### The robot's Wifi-Scanning Module
The wifi scanning module is basically the same as that from the localization lab. We simply removed all other
unecessary
components. It has an Wifi-enabled ESP32 connected to power and ground contacting our API endpoints on a consistent
basis.

### The robot's Driver Module
The robot's driver module is altogether different from anything in any lab. It is connected to a pair of motors which
are
connected to wheels, all on a chasis which also holds two battery packs: one for the the motors and one for the
Arduino. We have a motor driver to control the motors and an Arducam to take photos.

### Controller
The controller is similar to the ball-sim lab but with all unecessary components removed. It has a couple buttons
(though it only needs one) to enable or disable remote control. On top of this it has an IMU and an ESP32 with
ESP Now enabled.

## Hardware Layout
### The robot's Wifi-Scanning Module
TODO: there should be a system diagram here.

### The robot's Driver Module
TODO: there should be a system diagram here.

### Controller
TODO: there should be a system diagram here.

## Parts List
- Hobbyist Motors (from TA)
- Hobbyist Wheels (from TA)
- Hobbyist Chasis (from TA)
- AA Battery Pack (from TA)
- 4x AA 1.5V Batteries
- Dual Core ESP32S2 Arduino (3x)
- Camera (ArduCam)
- Sonar (HCSR04, used for aesthetics)
- 5v Battery Packs (2x)
- L298 Motor Driver
- IMU (MPU 6050)
- USB and jumper cables
- Button (From Lab)

## Total Parts Price
Around $63.31 excluding hardware from class and the motors, wheels, chasis,
and AA battery pack, which our TA, Amadou, was generously able to lend us
for the duration of this project.

## Other Components
- Server (software)
- ESP Now (built-in to the ESP32S2 Arduino)
- Dual Core Chip (built-in to the ESP32S2 Arduino)

## Design Challenges and Decisions
We faced various challenges throughout the design and implementation process.
Many of them we were forced to simply work around. They mainly included, but
were not limited those below.

### Implementation Challenges
**GPS not working.** We had troubles setting up our GPS. We followed the guide posted on the 6.08 class site, but
its
pins were outdated. After discussing with a TA, we were able to find the right pins, but even then it would not
display
anything. We tried all permutations of wiring for our GPS module, but were unable to fetch location information even
with it flashing as if it were functional. For that reason we abandoned GPS in favor of wifi-scanning.

**Wifi-scanning and requests latency; dual-core parallelism bugs with the Wifi Module.** Wifi-scanning from the lab
was
too slow for our needs, since it blocked the robot from acting on controller requests to change speed. For this reason
we
first tried using the dual-core functionality of the ESP32S2 Arduino, but it introduced parallelism bugs
sporadically
and unpredictably when the Wifi module did a scan to update its information regarding nearby access points (to query
Google's API with). This forced us to simply use a third Arduino for our location data collection.

**Initial lack of support for react-based web clients on the server.** We really wanted to have a reactive
web-client
for our application so that we could enable real-time data and control of the robot. However, we had so little control
over
the 608 server that at first it was impossible to easily package and serve web clients built with tools like React,
becuase it didn't support things such as serving images, which were necessary for our app to function. We tried to
package the app in a single file but it was more of a hassle than necessary, and after wrangling with the PUBLIC_URL
path in React's build toolchain we were able to serve React from the 608 server.

**Storage space for camera data both on on-board buffers and in the server.** Most image sizes were too big,
especially
in base64, to store on the Arduino and send to the server in one request. We also worried at first that these images
would not fit into our database. Fortunately, this was not the case and we were able to do both of these things by
picking the smallest possible image size, 160x120.

**Lack of power for a sensor-laden robot.** We were unable to do an end to end test until week 3 of the project
because
the robot had too many sensors to power them all in addition to the motors, even when given 6V from the battery pack
through
the motor driver. Luckily, our order of 5V USB power supplies arrived in time for us to power the Arduino
independently
with a power supply and thereby make our robot autonomous (disconneting it from the computer) and able to power its
sensors.

**JPEG HTML embedding platform differences.** We really struggled to display images because even while decoding them
into JPGs worked fine on our local computers (using the bash `base64` command), it did not on the server, where we
were
trying to serve an HTML page with base64-embedded JPEGs. After hours of debugging, with TA help, we ended up
clipping
arbitrary prefixes and suffixes from the base64 string and found a combination that worked by pure chance.

**Low Camera Framerate.** Because it took the Arduino a lot of work to take the pictures, encode them in base64,
send
them to the server, await an OK as the server input them into the database, and so forth, the frame rate for our
camera
is very low. It is enough for real-time control in static settings, but not viable for dynamic settings (i.e. with
many
moving objects). The TAs told us that this was as good as we were gonna get from the Arducam with the ESP32, but we
imagine that with better hardware and some performance optimization (and streaming) it would be possible to have a
more
real-time feed.

### Design Decisions
In addition to the above, we also had to scrap our wish to create gesture control since according to Joe it would be
too difficult. That information led us to choose tilt control since we had proven, in one of the labs, that it could
function reasonably well.

Our implementation challenges are what led us to choose wifi-scanning over GPS as our localization technique. We
understand that requiring Wifi is potentially not ideal for the sort of hypothetical scenarios in which a robot like
The robot would be useful, but understand that to create our MVP it was necessary to use the tools at our disposal.

# Detailed Code Layout
## Server
Our server code is seperated into two modules, called `Crud` and `Webpage`, which respectively handle API endpoints
to
read/write from/to the databases/state and a simple webpage server. We have a request handler which based on the URL
arguments passed is able to route the request responder to either Crud or the Webpage's handlers.

The Crud endpoints modify three databases: one for camera data, one for location data, and one for kinematic data.
We
have split it up this way because the modules handling the sending of these different types of data have different
frequencies at which they send their data. The Crud object implements entirely static methods which are typically
wrapped in a python SQLite connection and cursor decorator to modify the correct database. The following endpoints
are
available:

<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> Endpoint and Type</th>
            <th style="text-align:left"> URL Query String </th>
            <th style="text-align:left"> Description </th>
        </tr>
        <tr>
            <th style="text-align:left"> Kinematic and Location API GET </th>
            <th style="text-align:left"> Any Value other than "camera," "monalisa," "whereami," or "Wherehaveibeen"
            </th>
            <th style="text-align:left"> Get a list of historial location and kinematic data of the robot. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Kinematic and Location API POST </th>
            <th style="text-align:left"> None </th>
            <th style="text-align:left"> Post a new entry of location and kinematic data. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Whereami (GET) </th>
            <th style="text-align:left"> ?whereami=1 </th>
            <th style="text-align:left"> Find out where a latitude and longitude are at MIT. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Wherehaveibeen (GET) </th>
            <th style="text-align:left"> ?wherehaveibeen=1 </th>
            <th style="text-align:left"> Get a static HTML table of where the robot has been.</th>
        </tr>
        <tr>
            <th style="text-align:left"> Camera POST </th>
            <th style="text-align:left"> ?camera=1 </th>
            <th style="text-align:left"> Post a picture to the camera database. </th>
        </tr>
        <tr>
            <th style="text-align:left"> Camera GET </th>
            <th style="text-align:left"> ?camera=1 </th>
            <th style="text-align:left"> Get an HTML page with a picture from the latest camera feed. </th>
        </tr>
    </tbody>
</table>

The webpage only has one single meaningful endpoint to return the string representing the `index.html` of our
webpage.
We also have a dummy "mona lisa" endpoint accessible with `?monalisa=1` as a sample of HTML base64 embedding of
images.
Note that we had to put the wherehaveibeen endpoint in Crud and not Webpage because it requires reading from the
database directly. We implemented it as a low-level, simple MVP for localization early on, before we had map
waypoints.

Lastly, we have a GeoFencer object which encapsulates the functionality from the GeoFencing lab and allows us to
locate
the robot on MIT.

## Database Layout
### Location Database with Example
<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> "time_" Time (timestamp)</th>
            <th style="text-align:left"> "x_x" Latitude (real)</th>
            <th style="text-align:left"> "y_y" Longitude (real)</th>
            <th style="text-align:left"> "build" Building (text)</th>
        </tr>
        <tr>
            <th style="text-align:left"> 2022-05-08 08:15:27.243860 </th>
            <th style="text-align:left"> 42.360226</th>
            <th style="text-align:left"> -71.094139</th>
            <th style="text-align:left"> Student Center</th>
        </tr>
    </tbody>
</table>

### Camera Database with Example
<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> "time_" Time (timestamp)</th>
            <th style="text-align:left"> "image" Image Base64 (text)</th>
            <th style="text-align:left"> "response" Object Detection Class (text)</th>
        </tr>
        <tr>
            <th style="text-align:left"> 2022-05-08 08:15:27.243860</th>
            <th style="text-align:left"> /9j/... </th>
            <th style="text-align:left"> person </th>
        </tr>
    </tbody>
</table>

### Kinematics Database with Example
<table class="table">
    <tbody>
        <tr>
            <th style="text-align:left"> "time_" Time (timestamp)</th>
            <th style="text-align:left"> "a_x" Tilt x (real)</th>
            <th style="text-align:left"> "a_y" Tilt y (real)</th>
            <th style="text-align:left"> "v_x" Deprecated (real)</th>
            <th style="text-align:left"> "v_y" Deprecated (real)</th>
            <th style="text-align:left"> "speed" Speed (real)</th>
            <th style="text-align:left"> "direction" Direction (real)</th>
        </tr>
        <tr>
            <th style="text-align:left"> 2022-05-08 08:15:27.243860</th>
            <th style="text-align:left"> 0.534 </th>
            <th style="text-align:left"> -0.02 </th>
            <th style="text-align:left"> 0 </th>
            <th style="text-align:left"> 0 </th>
            <th style="text-align:left"> 75</th>
            <th style="text-align:left"> "1 </th>
        </tr>
    </tbody>
</table>

## Web Client
The web client has a browser router to allow users to select between the map of locations or a camera feed. These
can be
switched using a navigation bar at the top of the page. It was challenging integrating the navigation bar, not only
because React's built-in BrowserRouter did not work for some reason, but also because CSS was difficult when
integrating
the map from Google maps. Both the map and the camera are stateful and use polling to update their data to be up to
date.

## ESP32 Controller
The ESP32S2 controller is fortunately one of the simpler Arduino implementations in our project. Initially it was
doing
all the localization work because our Robot did not have enough power to use Wifi, but once we were able to shift
that
to be on-board in the robot, we were able to just handle ESP Now messaging. Every 80ms, our code simply gets the tilt
(corresponding to speed), processes it, then sends it to the robot (with some additional data like direction, angle, and
so
forth, though we only support the four main directions, forward, backwards, left and right, denoted by UP = 1, DOWN
= 2,
LEFT = 3, RIGHT = 4, for the purpose of keeping a de-noised experience; we also have NONE signifying not moving).

The `OnDataSent` function is a callback which ESP Now calls when it successfully sends a message. It is used to
inform
the user through the Serial console (if present) that data was successfully sent (which is important for debugging),
but
is otherwise not that important. The functions `get_direction`, `get_speed`, and `get_angle` turn tilt data into a
direction and a speed for that direction. As previously mentioned there are four directions for the four rays coming
out
of the origin of the four axis on the 2D plane corresponding to the birds-eye view of the ground. We also have a
functionality to print these kinematics data to the TFT so the user can know how they are conrolling the robot (for
example if it's far away).

We bin our speeds into 4 bins based on a linear mapping from the range of tilts (between 0 and 1G) and the range of
speeds (between 0 and 100 in a unit pre-determined by our motor driver). This is similar to the work done in the
Napster
lab for the notes, except the function is in fact purely linear.

## ESP32 Robot (Driver and Sensor)
The robot's code for the driver and sensor. TODO.

## ESP32 Robot (Wifi-Scanner/Locator)
The Wifi-scanner code is similar to the lab. It simply runs on a loop to scan wifi networks, build the query JSON,
and
then send it to the Google API at `https://www.googleapis.com/geolocation/v1/geolocate` with the key provided to us
in
one of the labs. Once it is done with this, it sends a request to our backend to update our locations database.

# Milestone Videos
## Week 1
### ESP Now
<a href="https://youtube.com/shorts/_qTj1VqcwLc?feature=share">Short is available here.</a>
For some reason Youtube kept insisting that we make this a short.

### Database
<a href="https://youtube.com/shorts/Jvd1F6fFxmg?feature=share">Short is available here.</a>
For some reason Youtube kept insisting that we make this a short.

### (Development) Website
<a href="https://youtube.com/shorts/j2ca18ya4Ws?feature=share">Short is available here.</a>
For some reason Youtube kept insisting that we make this a short.

### Typescript and Better Plotting
<!-- who the heck uses loom though -->
<div style="position: relative; padding-bottom: 64.5933014354067%; height: 0;"><iframe
        src="https://www.loom.com/embed/9cb2908149bb456d852604ae41a1b3b7" frameborder="0" webkitallowfullscreen
        mozallowfullscreen allowfullscreen
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>

## Week 2
### (Production) Website
<iframe width="560" height="315" src="https://www.youtube.com/embed/rjyxQVHdtcE" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

### Locating App in MIT using Wifi and Google API
<iframe width="560" height="315" src="https://www.youtube.com/embed/FPgzCNh0eX4" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

### Path on Webserver
<iframe width="560" height="315" src="https://www.youtube.com/embed/RTUxWQrvx-w" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

### Moving Robot's Wheels (Non-Autonomous)
This got like 400 views on Youtube.
<iframe
    src="https://player.vimeo.com/video/707954511?h=92e5081bfd&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
    width="325" height="578" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen
    title="wheels1.MOV"></iframe>

## Week 3
### Binned Speeds
<iframe width="560" height="315" src="https://www.youtube.com/embed/lkiynghh2tU" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

### Moving The robot (Autonomous)
<iframe
    src="https://player.vimeo.com/video/707954440?h=c64497cc93&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
    width="325" height="578" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen
    title="integrated.MOV"></iframe>

### React Router and Porting to React

<iframe src="https://drive.google.com/file/d/19HDeVBCRObiUB72gXjIIG6Xk1g50N2y0/preview" width="640" height="480"
    allow="autoplay"></iframe>

### Camera Feed On Website
<div style="position: relative; padding-bottom: 62.5%; height: 0;"><iframe
        src="https://www.loom.com/embed/e42725fd7e3c4c4d9e5d2d0830054df8" frameborder="0" webkitallowfullscreen
        mozallowfullscreen allowfullscreen
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>

<iframe src="https://drive.google.com/file/d/1iCJSLg03OSt5NRfpT1u9LspFbheg6mrJ/preview" width="640" height="480"
    allow="autoplay"></iframe>

## Week 4
### Robot Localization (from Robot) using GPS or Alternative Method
You can find our short <a href="https://www.youtube.com/shorts/DohrP9NixmE">here</a>.

### Dynamic Camera Feed
We have a dynamic camera feed available in the website we made. It is created by doing a, metaphorically speaking, inner
join between the camera and location data.

<div style="position: relative; padding-bottom: 62.5%; height: 0;"><iframe
        src="https://www.loom.com/embed/352e227e9c99414fab7271bbdc71d600" frameborder="0" webkitallowfullscreen
        mozallowfullscreen allowfullscreen
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>

We also have the integrated behavior here:
<iframe src="https://drive.google.com/file/d/1swHPeBE5sORZbmmahVKGt5c6eY5GmhDY/preview" width="640" height="480"
    allow="autoplay"></iframe>

### Paths Travelled on Database
<iframe width="560" height="315" src="https://www.youtube.com/embed/VZjazDh3RV4" title="YouTube video player"
    frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>

### Object Detection and Display
<iframe src="https://drive.google.com/file/d/1RZgU-t1SUoRDS1I6tiApG_-eogOuQYkN/preview" width="640" height="480"
    allow="autoplay"></iframe>

## End to End Integration: Using The Robot
TODO. This will come soon.

# Historical Notes
We initially thought of making a mostly web-based project involving a swiming mechanic to catch turtles on a web
client
by moving the arduino (sort of like nintendo kinect). The design of that idea is fleshed out in the project proposal
we
submitted 4 weeks ago. However, Joe and the TAs dissuaded us from pursuing such an idea because doing localization
of
the Arduino in 3D space using an IMU is an unsolved problem and beyond our scope. This is why we opted to instead to
tilt control for a robot.

# Team Members
- Natasha Maniar
- Adriano Hernandez
- Sualeh Asif
- Daniela Velez
<!-- Markdeep: -->
<style class="fallback">
    body {
        visibility: hidden;
        white-space: pre;
        font-family: monospace
    }
</style>
<script src="markdeep.min.js"></script>
<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script>
<script>window.alreadyProcessedMarkdeep || (document.body.style.visibility = "visible")</script>